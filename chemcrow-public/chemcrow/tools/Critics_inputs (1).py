import autogen
from langchain.tools import BaseTool
from pydantic import BaseModel, Field
from typing import List, Dict, Any

class ChemistryChatManagerConfig(BaseModel):
    model: str
    api_key: str

class ChemistryChatManager(BaseTool):
    name: str = "ChemistryChatManager"
    description: str = '''Used to verify the answer generated by other agents. Use this agent last to verify the final answer. It involves the student-critic-professor framework. Provide input in this format:

This question is asked by a chemistry student. The student should solve this question after critics inputs, and then the critic will verify and give inputs accordingly.

Question: "question"
Options: "solution/context till now" '''
    api_key: str = Field(..., description="API key for the LLM service")
    model: str = "gpt-4o"
    seed: int = 42
    temperature: float = 0.0
    config_list: List[ChemistryChatManagerConfig] = []
    llm_config: Dict[str, Any] = {}
    student: Any = None
    critic: Any = None
    user_proxy: Any = None

    def __init__(self, api_key: str, model: str = "gpt-4o", seed: int = 42, temperature: float = 0.0):
        super().__init__()
        self.api_key = api_key
        self.model = model
        self.seed = seed
        self.temperature = temperature
        self.config_list = [
            ChemistryChatManagerConfig(model=self.model, api_key=self.api_key)
        ]
        self.llm_config = {
            "seed": self.seed,
            "config_list": [c.dict() for c in self.config_list],
            "temperature": self.temperature
        }

        self.student = autogen.AssistantAgent(
            name="student",
            system_message='''You are a Chemistry student highly skilled in correcting the Chemistry solution based on the critic inputs. You give the final answer based on the errors provided by Critic Chemistry Professor of the solution. Your task is to understand the context and inputs from the critic and generate the final answer.''',
            llm_config=self.llm_config
        )

        self.critic = autogen.AssistantAgent(
            name="critic",
            system_message='''You are a critic Chemistry Professor renowned for your expertise in evaluating complex Chemistry solutions. Your role is to precisely check the student's work, identifying any errors, misconceptions, or areas for improvement. You should understand the question and student solution, providing detailed feedback on what is incorrect and offering guidance on how the student can correct and refine their solution. Avoid providing the final answer; instead, focus on giving inputs to correct the solution. If the student's solution is correct, simply respond with "Correct." Ensure your feedback leads to correct answer and reasoning''',
            llm_config=self.llm_config
        )

        self.user_proxy = autogen.UserProxyAgent(
            name="user_proxy",
            human_input_mode="NEVER",
            max_consecutive_auto_reply=5,
            is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
            code_execution_config={"work_dir": "groupchat", "use_docker": False},
            llm_config=self.llm_config,
            system_message="""Reply TERMINATE if the critic replied with "Correct" Otherwise, reply CONTINUE, or the reason why the task is not solved yet."""
        )

    def _run(self, question_prompt: str) -> str:
        print("question_prompt", question_prompt)
        groupchat = autogen.GroupChat(agents=[self.user_proxy, self.critic, self.student], messages=[], max_round=5)
        manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=self.llm_config)
        chat_history = self.user_proxy.initiate_chat(manager, message=question_prompt)

        # Combine chat history into a single string for output
        return "\n".join([message['content'] for message in chat_history])

    async def _arun(self, question_prompt: str) -> str:
        """Use the tool asynchronously."""
        raise NotImplementedError("Async not implemented.")
